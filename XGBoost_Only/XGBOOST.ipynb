{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 3, 30, 22, 14, 8, 5, 15, 16, 27, 12, 2, 19, 6, 1, 23, 9, 25, 29, 7]\n",
      "[11, 8, 3, 14, 30, 22, 7, 5, 15, 27, 19, 12, 6, 2, 1, 16, 23, 29, 25, 9]\n",
      "Accuracy: 15.00%\n",
      "NDCG@4: 0.80\n",
      "Mean Average Precision (up to rank 4): 0.67\n",
      "NDCG@10: 0.93\n",
      "Mean Average Precision (up to rank 10): 0.80\n",
      "NDCG@15: 0.93\n",
      "Mean Average Precision (up to rank 15): 0.82\n",
      "NDCG@20: 0.93\n",
      "Mean Average Precision (up to rank 20): 0.85\n"
     ]
    }
   ],
   "source": [
    "import np\n",
    "import pd\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "#2018/19\n",
    "with open('predicted_rankingsTrainingSetOne.txt', 'r') as f:\n",
    "    predicted_ranks_array = [int(line.strip()) for line in f.readlines()]\n",
    "print(predicted_ranks_array)\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "7,\n",
    "5,\n",
    "15,\n",
    "27,\n",
    "19,\n",
    "12,\n",
    "6,\n",
    "2,\n",
    "1,\n",
    "16,\n",
    "23,\n",
    "29,\n",
    "25,\n",
    "9\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "k = 4\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T09:42:39.830572800Z",
     "start_time": "2023-11-08T09:42:34.845630Z"
    }
   },
   "id": "391fc46bf6d90b94"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 3, 8, 30, 22, 14, 5, 7, 15, 16, 27, 2, 12, 19, 1, 6, 23, 28, 35, 10]\n",
      "[8, 11, 22, 3, 15, 14, 7, 30, 10, 1, 16, 5, 6, 12, 23, 27, 35, 2, 19, 28]\n",
      "Accuracy: 5.00%\n",
      "NDCG@5: 0.78\n",
      "Mean Average Precision (up to rank 5): 0.54\n",
      "NDCG@10: 0.79\n",
      "Mean Average Precision (up to rank 10): 0.68\n",
      "NDCG@15: 0.79\n",
      "Mean Average Precision (up to rank 15): 0.72\n",
      "NDCG@20: 0.79\n",
      "Mean Average Precision (up to rank 20): 0.77\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "#2019/20\n",
    "with open('predicted_rankingsTrainingSetTwo.txt', 'r') as f:\n",
    "    predicted_ranks_array = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "print(predicted_ranks_array)\n",
    "\n",
    "\n",
    "actual_rankings = [8,\n",
    "11,\n",
    "22,\n",
    "3,\n",
    "15,\n",
    "14,\n",
    "7,\n",
    "30,\n",
    "10,\n",
    "1,\n",
    "16,\n",
    "5,\n",
    "6,\n",
    "12,\n",
    "23,\n",
    "27,\n",
    "35,\n",
    "2,\n",
    "19,\n",
    "28\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):  \n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:26:53.386447200Z",
     "start_time": "2023-10-13T12:26:53.357514500Z"
    }
   },
   "id": "635727e7927c4624"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 3, 22, 30, 14, 7, 15, 5, 10, 16, 27, 12, 1, 6, 21, 23, 25, 35, 18]\n",
      "[11, 22, 8, 3, 15, 27, 14, 30, 18, 5, 35, 6, 7, 12, 16, 23, 1, 25, 21, 10]\n",
      "Accuracy: 10.00%\n",
      "NDCG@5: 0.95\n",
      "Mean Average Precision (up to rank 5): 0.79\n",
      "NDCG@10: 0.95\n",
      "Mean Average Precision (up to rank 10): 0.78\n",
      "NDCG@15: 0.96\n",
      "Mean Average Precision (up to rank 15): 0.78\n",
      "NDCG@20: 0.96\n",
      "Mean Average Precision (up to rank 20): 0.81\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "#2020/21\n",
    "with open('predicted_rankingsTrainingSetThree.txt', 'r') as f:\n",
    "    predicted_ranks_array = [int(line.strip()) for line in f.readlines()]\n",
    "    \n",
    "print(predicted_ranks_array)\n",
    "\n",
    "actual_rankings = [11,\n",
    "22,\n",
    "8,\n",
    "3,\n",
    "15,\n",
    "27,\n",
    "14,\n",
    "30,\n",
    "18,\n",
    "5,\n",
    "35,\n",
    "6,\n",
    "7,\n",
    "12,\n",
    "16,\n",
    "23,\n",
    "1,\n",
    "25,\n",
    "21,\n",
    "10\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:34:45.399140100Z",
     "start_time": "2023-10-13T12:34:45.355605800Z"
    }
   },
   "id": "36c6716c5896164d"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 3, 22, 30, 14, 18, 15, 7, 5, 27, 16, 12, 1, 6, 19, 23, 35, 28, 33]\n",
      "[11, 8, 3, 14, 30, 22, 27, 15, 23, 7, 6, 12, 33, 35, 16, 5, 18, 1, 19, 28]\n",
      "Accuracy: 25.00%\n",
      "NDCG@5: 0.97\n",
      "Mean Average Precision (up to rank 5): 0.91\n",
      "NDCG@10: 0.99\n",
      "Mean Average Precision (up to rank 10): 0.89\n",
      "NDCG@15: 0.99\n",
      "Mean Average Precision (up to rank 15): 0.85\n",
      "NDCG@20: 0.99\n",
      "Mean Average Precision (up to rank 20): 0.86\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "# Load the predicted rankings\n",
    "with open('predicted_rankingsTrainingSetFour.txt', 'r') as f:\n",
    "    predicted_ranks_array = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Print the loaded predicted rankings\n",
    "print(predicted_ranks_array)\n",
    "\n",
    "# Define the actual rankings\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "27,\n",
    "15,\n",
    "23,\n",
    "7,\n",
    "6,\n",
    "12,\n",
    "33,\n",
    "35,\n",
    "16,\n",
    "5,\n",
    "18,\n",
    "1,\n",
    "19,\n",
    "28\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "# Compute and print the accuracy\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Define relevance scores for DCG calculation\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T12:38:46.359428200Z",
     "start_time": "2023-10-13T12:38:46.324036300Z"
    }
   },
   "id": "de82caaef87f73fa"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 3, 22, 30, 14, 15, 7, 5, 27, 16, 18, 33, 12, 2, 6, 23, 35, 25, 32]\n",
      "[11, 30, 22, 6, 8, 23, 35, 14, 33, 25, 12, 3, 7, 27, 2, 32, 5, 15, 18, 16]\n",
      "Accuracy: 10.00%\n",
      "NDCG@5: 0.88\n",
      "Mean Average Precision (up to rank 5): 0.63\n",
      "NDCG@10: 0.87\n",
      "Mean Average Precision (up to rank 10): 0.61\n",
      "NDCG@15: 0.87\n",
      "Mean Average Precision (up to rank 15): 0.60\n",
      "NDCG@20: 0.90\n",
      "Mean Average Precision (up to rank 20): 0.67\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "# Load the predicted rankings\n",
    "with open('predicted_rankingsTrainingSetFive.txt', 'r') as f:\n",
    "    predicted_ranks_array = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Print the loaded predicted rankings\n",
    "print(predicted_ranks_array)\n",
    "\n",
    "# Define the actual rankings\n",
    "actual_rankings = [11,\n",
    "30,\n",
    "22,\n",
    "6,\n",
    "8,\n",
    "23,\n",
    "35,\n",
    "14,\n",
    "33,\n",
    "25,\n",
    "12,\n",
    "3,\n",
    "7,\n",
    "27,\n",
    "2,\n",
    "32,\n",
    "5,\n",
    "15,\n",
    "18,\n",
    "16\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "# Compute and print the accuracy\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Define relevance scores for DCG calculation\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_ranks_array, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:47:55.788340800Z",
     "start_time": "2023-10-13T19:47:55.766648200Z"
    }
   },
   "id": "9e9cd3e3f7bdec64"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'SNN_XGBoost_similarity_scores_trainingSetOne.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSNN_XGBoost_similarity_scores_trainingSetOne.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      2\u001B[0m     similarity_scoresTrainingSetOne \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mfloat\u001B[39m(line\u001B[38;5;241m.\u001B[39mstrip()[\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f\u001B[38;5;241m.\u001B[39mreadlines()]\n\u001B[0;32m      6\u001B[0m similarity_scoresTrainingSetOne\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'SNN_XGBoost_similarity_scores_trainingSetOne.txt'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('SNN_XGBoost_similarity_scores_trainingSetOne.txt', 'r') as f:\n",
    "    similarity_scoresTrainingSetOne = [float(line.strip()[1:-1]) for line in f.readlines()]\n",
    "\n",
    "\n",
    "\n",
    "similarity_scoresTrainingSetOne"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T12:19:48.632519100Z",
     "start_time": "2023-09-22T12:19:48.478617200Z"
    }
   },
   "id": "f390682300848f01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from creatingPairsForTrainingSetOne_SNNXGBoost import trainingSetOne\n",
    "combined_pairs_trainingSetOne, combined_labels_trainingSetOne, combined_pairs_trainingSetOneXGB, combined_team_pairs = trainingSetOne()\n",
    "\n",
    "combined_pairs_trainingSetOneXGB\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T09:25:13.706513500Z"
    }
   },
   "id": "fcc53b2e82c7bbef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from keras_tuner import Objective, RandomSearch, HyperModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_preprocessing import preprocess_data\n",
    "# Load datasets\n",
    "# files = [\n",
    "#     \"2012_13.xlsx\",\n",
    "#     \"2013_14.xlsx\",\n",
    "#     \"2014_15.xlsx\",\n",
    "#     \"2015_16.xlsx\",\n",
    "#     \"2016_17.xlsx\",\n",
    "#     \"2017_18.xlsx\",\n",
    "#     \"2018_19.xlsx\",\n",
    "#     \"2019_20.xlsx\",\n",
    "#     \"2020_21.xlsx\",\n",
    "#     \"2021_22.xlsx\",\n",
    "#     \"2022_23.xlsx\"\n",
    "# \n",
    "# ]\n",
    "# datasets = [pd.read_excel(file) for file in files]\n",
    "# # Extract numerical and categorical columns from the first dataset\n",
    "# numerical_cols_data = datasets[0].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# numerical_cols_data = [col for col in numerical_cols_data if col != \"team identifier\"]\n",
    "# \n",
    "# # Preprocess datasets\n",
    "# # Exclude 'Team' when extracting categorical columns\n",
    "# categorical_cols_data = datasets[0].select_dtypes(include=['object']).columns.tolist()\n",
    "# categorical_cols_data = [col for col in categorical_cols_data if col != \"Team\"]\n",
    "# \n",
    "# # Preprocess datasets\n",
    "# preprocessed_datasets = [preprocess_data(df, numerical_cols_data, categorical_cols_data) for df in datasets]\n",
    "# \n",
    "# # Check columns in the last preprocessed dataset\n",
    "# # print(preprocessed_datasets[-1].columns)\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Combine the first six datasets for training\n",
    "# train_data = pd.concat(preprocessed_datasets[:-1], ignore_index=True)\n",
    "# \n",
    "# # Extract team identifiers and rankings from the seventh dataset for testing\n",
    "# test_data = preprocessed_datasets[-1][[\"Team\", \"team identifier\"]].copy()\n",
    "# test_data[\"Rank\"] = test_data.index + 1\n",
    "# print(test_data[\"team identifier\"])\n",
    "# # Normalize the features in the training data\n",
    "# \n",
    "# \n",
    "# # Aggregate features by team identifier\n",
    "# aggregated_data = train_data.groupby(\"team identifier\").mean().reset_index()\n",
    "# # Prepare pairwise comparison data for aggregated data\n",
    "# all_pairwise_data = []\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Creating pairwise ranking data for each season separately.\n",
    "# # ...\n",
    "# \n",
    "# for dataset in preprocessed_datasets[:-1]:  # Exclude the last dataset (test dataset)\n",
    "#     # Drop non-numeric columns\n",
    "#     numeric_dataset = dataset.select_dtypes(include=['int64', 'float64']).drop(\"team identifier\", axis=1)\n",
    "# \n",
    "#     for i, j in combinations(numeric_dataset.index, 2):\n",
    "#         feature_diff = numeric_dataset.iloc[i] - numeric_dataset.iloc[j]\n",
    "# \n",
    "#         # If i < j, then label = 1, indicating team i is ranked higher than team j\n",
    "#         label = 1 if i < j else 0\n",
    "#         all_pairwise_data.append((feature_diff, label))\n",
    "# \n",
    "#         # Data Augmentation: Add the reverse pair with the opposite label\n",
    "#         feature_diff_reverse = numeric_dataset.iloc[j] - numeric_dataset.iloc[i]\n",
    "#         label_reverse = 1 - label\n",
    "#         all_pairwise_data.append((feature_diff_reverse, label_reverse))\n",
    "# \n",
    "# # ...\n",
    "\n",
    "pairwise_y = combined_labels_trainingSetOne\n",
    "pairwise_X = pd.DataFrame(similarity_scoresTrainingSetOne, columns=[\"similarity_scores\"])\n",
    "\n",
    "# pairwise_X = pd.DataFrame([item[0] for item in all_pairwise_data])\n",
    "# pairwise_y = [item[1] for item in all_pairwise_data]\n",
    "# print(pairwise_y)\n",
    "\n",
    "# Split the data and convert it to DMatrix\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    pairwise_X,\n",
    "    pairwise_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "\n",
    "# Define the XGBoost hypermodel to only build the parameter dictionary\n",
    "class XGBHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        param = {\n",
    "            'max_depth': hp.Int('max_depth', 3, 10, 1),\n",
    "            'eta': hp.Float('eta', 0.01, 0.5, step=0.01),\n",
    "            'subsample': hp.Float('subsample', 0.5, 1),\n",
    "            'colsample_bytree': hp.Float('colsample_bytree', 0.5, 1),\n",
    "            'gamma': hp.Float('gamma', 0, 5),\n",
    "            'min_child_weight': hp.Int('min_child_weight', 1, 10),\n",
    "            'lambda': hp.Float('lambda', 0.01, 1),\n",
    "            'alpha': hp.Float('alpha', 0.01, 1),\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        return param\n",
    "\n",
    "# Define the custom tuner\n",
    "class XGBTuner(RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        params = self.hypermodel.build(hp)\n",
    "\n",
    "        # Train the model and get evaluation results\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(params, dtrain, evals=[(dval, 'eval')],\n",
    "                        early_stopping_rounds=10, verbose_eval=False, evals_result=evals_result)\n",
    "\n",
    "        # Get the last evaluation result\n",
    "        last_eval = evals_result['eval']['logloss'][-1]  # Changed rmse to logloss\n",
    "\n",
    "        # Report the result to the tuner\n",
    "        self.oracle.update_trial(trial.trial_id, {'val_logloss': last_eval})  # Changed val_rmse to val_logloss\n",
    "        self.save_model(trial.trial_id, bst)\n",
    "\n",
    "    def save_model(self, trial_id, model, step=0):\n",
    "        fname = os.path.join(self.get_trial_dir(trial_id), f'model_{step}.xgb')\n",
    "        model.save_model(fname)\n",
    "\n",
    "    def load_model(self, trial_id, step=0):\n",
    "        fname = os.path.join(self.get_trial_dir(trial_id), f'model_{step}.xgb')\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(fname)\n",
    "        return model\n",
    "\n",
    "# Use the custom tuner to search for the best hyperparameters\n",
    "tuner = XGBTuner(\n",
    "    XGBHyperModel(),\n",
    "    objective=Objective('val_logloss', direction='min'),\n",
    "    max_trials=50,\n",
    "    directory='xgb_tuner',\n",
    "    project_name='xgb_tuning'\n",
    ")\n",
    "\n",
    "tuner.search()\n",
    "# Display the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best max_depth:\", best_hp.get('max_depth'))\n",
    "print(\"Best eta:\", best_hp.get('eta'))\n",
    "print(\"Best subsample:\", best_hp.get('subsample'))\n",
    "print(\"Best colsample_bytree:\", best_hp.get('colsample_bytree'))\n",
    "print(\"Best gamma:\", best_hp.get('gamma'))\n",
    "print(\"Best min_child_weight:\", best_hp.get('min_child_weight'))\n",
    "print(\"Best lambda (L2 regularization):\", best_hp.get('lambda'))\n",
    "print(\"Best alpha (L1 regularization):\", best_hp.get('alpha'))\n",
    "best_params = {\n",
    "    'max_depth': best_hp.get('max_depth'),\n",
    "    'eta': best_hp.get('eta'),\n",
    "    'subsample': best_hp.get('subsample'),\n",
    "    'colsample_bytree': best_hp.get('colsample_bytree'),\n",
    "    'gamma': best_hp.get('gamma'),\n",
    "    'min_child_weight': best_hp.get('min_child_weight'),\n",
    "    'lambda': best_hp.get('lambda'),\n",
    "    'alpha': best_hp.get('alpha'),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "num_round = 60\n",
    "bst_aggregated = xgb.train(best_params, dtrain, num_round, evals=[(dval, 'eval')], early_stopping_rounds=10)\n",
    "\n",
    "bst_aggregated.save_model('SNN_XGBoosttrainingSetOne.xgb')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T09:25:13.710522100Z",
     "start_time": "2023-09-22T09:25:13.710522100Z"
    }
   },
   "id": "8995e785b41cd067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from keras_tuner import Objective, RandomSearch, HyperModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_preprocessing import preprocess_data\n",
    "from creatingPairsForTrainingSetOne_SNNXGBoost import trainingSetOne\n",
    "\n",
    "def predict_rankings_using_similarity(model, test_data, similarity_scores, combined_team_pairs):\n",
    "    \"\"\"Predict rankings using the provided model, test data, and similarity scores.\"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        team = test_data.iloc[i][\"team identifier\"]\n",
    "        \n",
    "        print(f\"Processing team: {test_data.iloc[i]['Team']} with identifier: {team}\")\n",
    "\n",
    "        # Retrieve similarity scores for the current team against all other teams\n",
    "        team_similarity_scores = []\n",
    "        for j, (team_a, team_b) in enumerate(combined_team_pairs):\n",
    "            if team == team_a or team == team_b:\n",
    "                team_similarity_scores.append(similarity_scores[j])\n",
    "\n",
    "        # If no similarity scores are found, append a default score (e.g., 0.0)\n",
    "        if not team_similarity_scores:\n",
    "            print(f\"No similarity scores found for team: {team}\")\n",
    "            scores.append(0.0)\n",
    "        else:\n",
    "            dtest_team_similarity = xgb.DMatrix(pd.DataFrame(team_similarity_scores, columns=[\"similarity\"]))\n",
    "            preds_team = model.predict(dtest_team_similarity)\n",
    "            scores.append(np.sum(preds_team))\n",
    "\n",
    "    # Rank the teams based on the predicted scores\n",
    "    test_data[\"Predicted Score\"] = scores\n",
    "    test_data = test_data.sort_values(by=\"Predicted Score\", ascending=False)\n",
    "    test_data[\"Predicted Rank\"] = range(1, len(test_data) + 1)\n",
    "\n",
    "    ordered_by_identifier = test_data.sort_values(by=\"team identifier\")\n",
    "\n",
    "    # Extract the 'Predicted Rank' column as an array\n",
    "    predicted_ranks_array = test_data[\"team identifier\"].to_numpy()\n",
    "\n",
    "    return test_data[[\"Team\", \"Rank\", \"Predicted Rank\"]], predicted_ranks_array\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "files = [\n",
    "        \"2012_13.xlsx\",\n",
    "        \"2013_14.xlsx\",\n",
    "        \"2014_15.xlsx\",\n",
    "        \"2015_16.xlsx\",\n",
    "        \"2016_17.xlsx\",\n",
    "        \"2017_18.xlsx\",\n",
    "        \"2018_19.xlsx\"\n",
    "    ]\n",
    "datasets = [pd.read_excel(file) for file in files]\n",
    "\n",
    "    # Preprocess datasets\n",
    "numerical_cols_data = datasets[0].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols_data = [col for col in numerical_cols_data if col != \"team identifier\"]\n",
    "categorical_cols_data = datasets[0].select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols_data = [col for col in categorical_cols_data if col != \"Team\"]\n",
    "preprocessed_datasets = [preprocess_data(df, numerical_cols_data, categorical_cols_data) for df in datasets]\n",
    "\n",
    "    # Prepare test data\n",
    "test_data = preprocessed_datasets[-1][[\"Team\", \"team identifier\"]].copy()\n",
    "test_data[\"Rank\"] = test_data.index + 1\n",
    "\n",
    "\n",
    "combined_pairs_trainingSetOne, combined_labels_trainingSetOne, combined_pairs_trainingSetOneXGB, combined_team_pairs = trainingSetOne()\n",
    "\n",
    "\n",
    "\n",
    "with open('SNN_XGBoost_similarity_scores_trainingSetOne.txt', 'r') as f:\n",
    "    similarity_scoresTrainingSetOne = [float(line.strip()[1:-1]) for line in f.readlines()]\n",
    "\n",
    "# Demonstration with mocked data\n",
    "\n",
    "# Mocked model (for demonstration purposes)\n",
    "bst = xgb.Booster()\n",
    "bst.load_model('SNN_XGBoosttrainingSetOne.xgb')  # Placeholder; replace with your actual model path\n",
    "\n",
    "predicted_rankings, predicted_ranks_array = predict_rankings_using_similarity(\n",
    "    bst,\n",
    "    test_data,\n",
    "    similarity_scoresTrainingSetOne,\n",
    "    combined_team_pairs\n",
    ")\n",
    "predicted_ranks_array\n",
    "\n",
    "\n",
    "print(predicted_ranks_array)\n",
    "\n",
    "print(predicted_rankings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T09:25:13.713053300Z"
    }
   },
   "id": "fce1a033e911d81c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import np\n",
    "import pd\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "#2018/19\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "7,\n",
    "5,\n",
    "15,\n",
    "27,\n",
    "19,\n",
    "12,\n",
    "6,\n",
    "2,\n",
    "1,\n",
    "16,\n",
    "23,\n",
    "29,\n",
    "25,\n",
    "9\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "def average_precision_from_relevance(relevance):\n",
    "    \"\"\"Compute Average Precision (AP) from relevance values.\"\"\"\n",
    "    precision_k = [sum(relevance[:k+1]) / (k+1) for k in range(len(relevance))]\n",
    "    return sum(precision_k) / len(precision_k)\n",
    "\n",
    "# Compute relevance values\n",
    "relevance_values_continuous = compute_relevance(predicted_ranks_array, actual_rankings)\n",
    "\n",
    "# Compute MAP\n",
    "map_value = average_precision_from_relevance(relevance_values_continuous)\n",
    "\n",
    "\n",
    "\n",
    "k = 20\n",
    "\n",
    "# def mean_reciprocal_rank(predicted_rankings, actual_rankings):\n",
    "#     \"\"\"Compute the Mean Reciprocal Rank (MRR).\"\"\"\n",
    "#     ranks = []\n",
    "#     for actual_team in actual_rankings:\n",
    "#         predicted_rank = predicted_rankings.index(actual_team) + 1\n",
    "#         ranks.append(predicted_rank)\n",
    "# \n",
    "#     return sum([1/r for r in ranks]) / len(ranks)\n",
    "def mean_reciprocal_rank(actual, predicted):\n",
    "    \"\"\"Compute the Mean Reciprocal Rank (MRR) using binary relevance.\"\"\"\n",
    "    for i, (a, p) in enumerate(zip(actual, predicted)):\n",
    "        if a == p:\n",
    "            return 1 / (i + 1)\n",
    "    return 0  # If no correct predictions\n",
    "\n",
    "\n",
    "# Compute MRR\n",
    "mrr_value = mean_reciprocal_rank(predicted_ranks_array, actual_rankings)\n",
    "tau, p_value = kendalltau(predicted_ranks_array, actual_rankings)\n",
    "print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'MAP: {map_value:.2f}')\n",
    "print(f'MRR:{mrr_value:.2f}')\n",
    "print(f\"Kendall's Tau: {tau:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T09:25:13.716693900Z",
     "start_time": "2023-09-22T09:25:13.716693900Z"
    }
   },
   "id": "f572b60bb191f5a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from keras_tuner import Objective, RandomSearch, HyperModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_preprocessing import preprocess_data\n",
    "# Load datasets\n",
    "from itertools import combinations\n",
    "\n",
    "files = [\n",
    "        \"2012_13.xlsx\",\n",
    "        \"2013_14.xlsx\",\n",
    "        \"2014_15.xlsx\",\n",
    "        \"2015_16.xlsx\",\n",
    "        \"2016_17.xlsx\",\n",
    "        \"2017_18.xlsx\",\n",
    "        \"2018_19.xlsx\"\n",
    "    ]\n",
    "datasets = [pd.read_excel(file) for file in files]\n",
    "\n",
    "    # Preprocess datasets\n",
    "numerical_cols_data = datasets[0].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols_data = [col for col in numerical_cols_data if col != \"team identifier\"]\n",
    "categorical_cols_data = datasets[0].select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols_data = [col for col in categorical_cols_data if col != \"Team\"]\n",
    "preprocessed_datasets = [preprocess_data(df, numerical_cols_data, categorical_cols_data) for df in datasets]\n",
    "\n",
    "    # Prepare test data\n",
    "test_data = preprocessed_datasets[-1][[\"Team\", \"team identifier\"]].copy()\n",
    "test_data[\"Rank\"] = test_data.index + 1\n",
    "\n",
    "    # Aggregate features by team identifier\n",
    "train_data = pd.concat(preprocessed_datasets[:-1], ignore_index=True)\n",
    "aggregated_data = train_data.groupby(\"team identifier\").mean().reset_index()\n",
    "\n",
    "predicted_wins = {team: 0 for team in aggregated_data['team identifier']}\n",
    "for i, (team1, team2) in enumerate(combinations(aggregated_data['team identifier'], 2)):\n",
    "    if similarity_scoresTrainingSetOne[i] > 1:\n",
    "        predicted_wins[team1] += similarity_scoresTrainingSetOne[i]\n",
    "    else:\n",
    "        predicted_wins[team2] -= similarity_scoresTrainingSetOne[i]\n",
    "\n",
    "team_wins = pd.DataFrame(list(map(list, zip(predicted_wins.values()))), index=predicted_wins.keys(), columns=['predicted_wins'])\n",
    "print(team_wins)\n",
    "rankings = team_wins.sort_values('predicted_wins')\n",
    "aggregated_data = aggregated_data.merge(team_wins, left_on=\"team identifier\", right_index=True)\n",
    "scaling_factor = 1.25  # Or any other number\n",
    "aggregated_data['predicted_wins'] = aggregated_data['predicted_wins'] * scaling_factor\n",
    "\n",
    "\n",
    "all_pairwise_data_updated = []\n",
    "\n",
    "for dataset in preprocessed_datasets[:-1]:  # Exclude the last dataset (test dataset)\n",
    "    # Drop non-numeric columns\n",
    "    numeric_dataset = dataset.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # Merge with aggregated_data to get 'predicted_wins' column\n",
    "    numeric_dataset = numeric_dataset.merge(aggregated_data[['team identifier', 'predicted_wins']], \n",
    "                                            left_on=\"team identifier\", \n",
    "                                            right_on=\"team identifier\", \n",
    "                                            how='left')\n",
    "\n",
    "    for i, j in combinations(numeric_dataset.index, 2):\n",
    "        feature_diff = numeric_dataset.iloc[i] - numeric_dataset.iloc[j]\n",
    "\n",
    "        # If i < j, then label = 1, indicating team i is ranked higher than team j\n",
    "        label = 1 if i < j else 0\n",
    "        all_pairwise_data_updated.append((feature_diff, label))\n",
    "\n",
    "        # Data Augmentation: Add the reverse pair with the opposite label\n",
    "        feature_diff_reverse = numeric_dataset.iloc[j] - numeric_dataset.iloc[i]\n",
    "        label_reverse = 1 - label\n",
    "        all_pairwise_data_updated.append((feature_diff_reverse, label_reverse))\n",
    "\n",
    "# Create the updated pairwise dataset\n",
    "pairwise_X_updated = pd.DataFrame([item[0] for item in all_pairwise_data_updated])\n",
    "\n",
    "pairwise_y = [item[1] for item in all_pairwise_data]\n",
    "print(\"pairwise_X:  \",pairwise_X.shape)\n",
    "print(\"pairwise_y:   \", len(pairwise_y))\n",
    "# Split the data and convert it to DMatrix\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    pairwise_X,\n",
    "    pairwise_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Define the XGBoost hypermodel to only build the parameter dictionary\n",
    "class XGBHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        param = {\n",
    "            'max_depth': hp.Int('max_depth', 3, 10, 1),\n",
    "            'eta': hp.Float('eta', 0.01, 0.5, step=0.01),\n",
    "            'subsample': hp.Float('subsample', 0.5, 1),\n",
    "            'colsample_bytree': hp.Float('colsample_bytree', 0.5, 1),\n",
    "            'gamma': hp.Float('gamma', 0, 5),\n",
    "            'min_child_weight': hp.Int('min_child_weight', 1, 10),\n",
    "            'lambda': hp.Float('lambda', 0.01, 1),\n",
    "            'alpha': hp.Float('alpha', 0.01, 1),\n",
    "            'objective': 'rank:pairwise',\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        return param\n",
    "\n",
    "# Define the custom tuner\n",
    "class XGBTuner(RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        params = self.hypermodel.build(hp)\n",
    "\n",
    "        # Train the model and get evaluation results\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(params, dtrain, evals=[(dval, 'eval')],\n",
    "                        early_stopping_rounds=10, verbose_eval=False, evals_result=evals_result)\n",
    "\n",
    "        # Get the last evaluation result\n",
    "        last_eval = evals_result['eval']['logloss'][-1]  # Changed rmse to logloss\n",
    "\n",
    "        # Report the result to the tuner\n",
    "        self.oracle.update_trial(trial.trial_id, {'val_logloss': last_eval})  # Changed val_rmse to val_logloss\n",
    "        self.save_model(trial.trial_id, bst)\n",
    "\n",
    "    def save_model(self, trial_id, model, step=0):\n",
    "        fname = os.path.join(self.get_trial_dir(trial_id), f'model_{step}.xgb')\n",
    "        model.save_model(fname)\n",
    "\n",
    "    def load_model(self, trial_id, step=0):\n",
    "        fname = os.path.join(self.get_trial_dir(trial_id), f'model_{step}.xgb')\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(fname)\n",
    "        return model\n",
    "\n",
    "# Use the custom tuner to search for the best hyperparameters\n",
    "tuner = XGBTuner(\n",
    "    XGBHyperModel(),\n",
    "    objective=Objective('val_logloss', direction='min'),\n",
    "    max_trials=50,\n",
    "    directory='xgb_tuner',\n",
    "    project_name='xgb_tuning'\n",
    ")\n",
    "\n",
    "tuner.search()\n",
    "# Display the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best max_depth:\", best_hp.get('max_depth'))\n",
    "print(\"Best eta:\", best_hp.get('eta'))\n",
    "print(\"Best subsample:\", best_hp.get('subsample'))\n",
    "print(\"Best colsample_bytree:\", best_hp.get('colsample_bytree'))\n",
    "print(\"Best gamma:\", best_hp.get('gamma'))\n",
    "print(\"Best min_child_weight:\", best_hp.get('min_child_weight'))\n",
    "print(\"Best lambda (L2 regularization):\", best_hp.get('lambda'))\n",
    "print(\"Best alpha (L1 regularization):\", best_hp.get('alpha'))\n",
    "best_params = {\n",
    "    'max_depth': best_hp.get('max_depth'),\n",
    "    'eta': best_hp.get('eta'),\n",
    "    'subsample': best_hp.get('subsample'),\n",
    "    'colsample_bytree': best_hp.get('colsample_bytree'),\n",
    "    'gamma': best_hp.get('gamma'),\n",
    "    'min_child_weight': best_hp.get('min_child_weight'),\n",
    "    'lambda': best_hp.get('lambda'),\n",
    "    'alpha': best_hp.get('alpha'),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "num_round = 60\n",
    "bst_aggregated = xgb.train(best_params, dtrain, num_round, evals=[(dval, 'eval')], early_stopping_rounds=10)\n",
    "\n",
    "\n",
    "\n",
    "bst_aggregated.save_model('SNN_XGBoosttrainingSetOne.xgb')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T09:25:13.720961400Z"
    }
   },
   "id": "eec5ab8dcc00a9bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from data_preprocessing import preprocess_data\n",
    "\n",
    "\n",
    "def load_best_model(model_path):\n",
    "    \"\"\"Load the saved XGBoost model.\"\"\"\n",
    "    bst = xgb.Booster()\n",
    "    bst.load_model(model_path)\n",
    "    return bst\n",
    "\n",
    "\n",
    "def predict_rankings(model, test_data, aggregated_data):\n",
    "    \"\"\"Predict rankings using the provided model and test data.\"\"\"\n",
    "    scores = []\n",
    "    for i in range(len(test_data)):\n",
    "        team_id = test_data.iloc[i][\"team identifier\"]\n",
    "        team_data_aggregated = aggregated_data[aggregated_data[\"team identifier\"] == team_id]\n",
    "\n",
    "        if not team_data_aggregated.empty:\n",
    "            team_features = team_data_aggregated.iloc[0].drop(\"team identifier\")\n",
    "            pairwise_data_temp = [team_features - aggregated_data.iloc[j].drop(\"team identifier\") for j in\n",
    "                                  range(len(aggregated_data))]\n",
    "            pairwise_X_temp = pd.DataFrame(pairwise_data_temp)\n",
    "            \n",
    "            print(\"Columns in pairwise_X_temp:\", pairwise_X_temp.columns)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "     \n",
    "                        \n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "            dtest_pairwise_temp = xgb.DMatrix(pairwise_X_temp)\n",
    "            # print(\"dtest_pairwise_temp:   \", dtest_pairwise_temp)\n",
    "            preds_temp = model.predict(dtest_pairwise_temp)\n",
    "       \n",
    "            scores.append(np.mean(preds_temp))\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "    \n",
    "    # Rank the teams based on the aggregated scores\n",
    "    test_data[\"Predicted Score\"] = scores\n",
    "    test_data = test_data.sort_values(by=\"Predicted Score\", ascending=False)\n",
    "    test_data[\"Predicted Rank\"] = range(1, len(test_data) + 1)\n",
    "\n",
    "    print(test_data[[\"Team\", \"Predicted Score\", \"Predicted Rank\"]])\n",
    "\n",
    "    \n",
    "\n",
    "    # Extract the 'Predicted Rank' column as an array\n",
    "    predicted_ranks_array = test_data[\"team identifier\"].to_numpy()\n",
    "\n",
    "    return test_data[[\"Team\", \"Rank\", \"Predicted Rank\"]], predicted_ranks_array\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load datasets\n",
    "\n",
    "    files = [\n",
    "            \"2012_13.xlsx\",\n",
    "            \"2013_14.xlsx\",\n",
    "            \"2014_15.xlsx\",\n",
    "            \"2015_16.xlsx\",\n",
    "            \"2016_17.xlsx\",\n",
    "            \"2017_18.xlsx\",\n",
    "            \"2018_19.xlsx\"\n",
    "        ]\n",
    "    datasets = [pd.read_excel(file) for file in files]\n",
    "    \n",
    "        # Preprocess datasets\n",
    "    numerical_cols_data = datasets[0].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    numerical_cols_data = [col for col in numerical_cols_data if col != \"team identifier\"]\n",
    "    categorical_cols_data = datasets[0].select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols_data = [col for col in categorical_cols_data if col != \"Team\"]\n",
    "    preprocessed_datasets = [preprocess_data(df, numerical_cols_data, categorical_cols_data) for df in datasets]\n",
    "    \n",
    "        # Prepare test data\n",
    "    test_data = preprocessed_datasets[-1][[\"Team\", \"team identifier\"]].copy()\n",
    "    test_data[\"Rank\"] = test_data.index + 1\n",
    "    # print(\"test_data:   \",test_data)\n",
    "    \n",
    "        # Aggregate features by team identifier\n",
    "    train_data = pd.concat(preprocessed_datasets[:-1], ignore_index=True)\n",
    "    aggregated_data = train_data.groupby(\"team identifier\").mean().reset_index()\n",
    "    \n",
    "    predicted_wins = {team: 0 for team in aggregated_data['team identifier']}\n",
    "    for i, (team1, team2) in enumerate(combinations(aggregated_data['team identifier'], 2)):\n",
    "        if similarity_scoresTrainingSetOne[i] > 1:\n",
    "            predicted_wins[team1] += similarity_scoresTrainingSetOne[i]\n",
    "        else:\n",
    "            predicted_wins[team2] -= similarity_scoresTrainingSetOne[i]\n",
    "    \n",
    "    team_wins = pd.DataFrame(list(map(list, zip(predicted_wins.values()))), index=predicted_wins.keys(), columns=['predicted_wins'])\n",
    "    rankings = team_wins.sort_values('predicted_wins')\n",
    "    aggregated_data = aggregated_data.merge(team_wins, left_on=\"team identifier\", right_index=True)\n",
    "   \n",
    "    # print(\"Columns in aggregated_data:\", aggregated_data.columns)\n",
    "    # \n",
    "    \n",
    "    # print(\"shape:  \", aggregated_data)\n",
    "    # Load the best model\n",
    "    bst = load_best_model('SNN_XGBoosttrainingSetOne.xgb')\n",
    "    print(\"Features in the trained model: \", bst.feature_names)\n",
    "\n",
    "    # Predict rankings\n",
    "    predicted_rankings, predicted_ranks_array = predict_rankings(bst, test_data, aggregated_data)\n",
    "    # print(predicted_rankings)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # print(predicted_ranks_array)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T09:25:13.730140700Z",
     "start_time": "2023-09-22T09:25:13.724131600Z"
    }
   },
   "id": "a5b08917732fbe7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n",
    "from math import log2\n",
    "\n",
    "#2018/19\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "7,\n",
    "5,\n",
    "15,\n",
    "27,\n",
    "19,\n",
    "12,\n",
    "6,\n",
    "2,\n",
    "1,\n",
    "16,\n",
    "23,\n",
    "29,\n",
    "25,\n",
    "9\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "print(predicted_ranks_array)\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = np.where(predicted_rankings == predicted_team)[0][0]\n",
    "        actual_rank = np.where(actual_rankings == predicted_team)[0][0]\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rank_difference = abs(predicted_rank - actual_rank)\n",
    "        relevance.append(1 / (1 + rank_difference))\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def average_precision_from_relevance(relevance):\n",
    "    \"\"\"Compute Average Precision (AP) from relevance values.\"\"\"\n",
    "    precision_k = [sum(relevance[:k+1]) / (k+1) for k in range(len(relevance))]\n",
    "    return sum(precision_k) / len(precision_k)\n",
    "\n",
    "# Compute relevance values\n",
    "relevance_values_continuous = compute_relevance(predicted_ranks_array, actual_rankings)\n",
    "\n",
    "# Compute MAP\n",
    "map_value = average_precision_from_relevance(relevance_values_continuous)\n",
    "\n",
    "\n",
    "\n",
    "k = 20\n",
    "\n",
    "def mean_reciprocal_rank(predicted_rankings, actual_rankings):\n",
    "    ranks = []\n",
    "    for actual_team in actual_rankings:\n",
    "        predicted_rank = np.where(predicted_rankings == actual_team)[0][0] + 1\n",
    "        ranks.append(predicted_rank)\n",
    "    return sum([1/r for r in ranks]) / len(ranks)\n",
    "\n",
    "\n",
    "\n",
    "# Compute MRR\n",
    "mrr_value = mean_reciprocal_rank(predicted_ranks_array, actual_rankings)\n",
    "tau, p_value = kendalltau(predicted_ranks_array, actual_rankings)\n",
    "print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'MAP: {map_value:.2f}')\n",
    "print(f'MRR:{mrr_value:.2f}')\n",
    "print(f\"Kendall's Tau: {tau:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T09:25:13.726140500Z"
    }
   },
   "id": "661a91a9d4f0f9ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from keras_tuner import Objective, RandomSearch, HyperModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data_preprocessing import preprocess_data\n",
    "# Load datasets\n",
    "files = [\n",
    "    \"2012_13.xlsx\",\n",
    "    \"2013_14.xlsx\",\n",
    "    \"2014_15.xlsx\",\n",
    "    \"2015_16.xlsx\",\n",
    "    \"2016_17.xlsx\",\n",
    "    \"2017_18.xlsx\",\n",
    "    \"2018_19.xlsx\",\n",
    "    \"2019_20.xlsx\",\n",
    "    \"2020_21.xlsx\",\n",
    "    \"2021_22.xlsx\",\n",
    "    \"2022_23.xlsx\"\n",
    "\n",
    "]\n",
    "datasets = [pd.read_excel(file) for file in files]\n",
    "# Extract numerical and categorical columns from the first dataset\n",
    "numerical_cols_data = datasets[0].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols_data = [col for col in numerical_cols_data if col != \"team identifier\"]\n",
    "\n",
    "# Preprocess datasets\n",
    "# Exclude 'Team' when extracting categorical columns\n",
    "categorical_cols_data = datasets[0].select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols_data = [col for col in categorical_cols_data if col != \"Team\"]\n",
    "\n",
    "# Preprocess datasets\n",
    "preprocessed_datasets = [preprocess_data(df, numerical_cols_data, categorical_cols_data) for df in datasets]\n",
    "predicted_wins = {team: 0 for team in aggregated_data['team identifier']}\n",
    "for i, (team1, team2) in enumerate(combinations(aggregated_data['team identifier'], 2)):\n",
    "    if similarity_scoresTrainingSetOne[i] > 1:\n",
    "        predicted_wins[team1] += similarity_scoresTrainingSetOne[i]\n",
    "    else:\n",
    "        predicted_wins[team2] -= similarity_scoresTrainingSetOne[i]\n",
    "\n",
    "team_wins = pd.DataFrame(list(map(list, zip(predicted_wins.values()))), index=predicted_wins.keys(), columns=['predicted_wins'])\n",
    "print(team_wins)\n",
    "rankings = team_wins.sort_values('predicted_wins')\n",
    "aggregated_data = aggregated_data.merge(team_wins, left_on=\"team identifier\", right_index=True)\n",
    "scaling_factor = 1.25  # Or any other number\n",
    "aggregated_data['predicted_wins'] = aggregated_data['predicted_wins'] * scaling_factor\n",
    "\n",
    "train_data =team_wins\n",
    "\n",
    "# Extract team identifiers and rankings from the seventh dataset for testing\n",
    "test_data = preprocessed_datasets[-1][[\"Team\", \"team identifier\"]].copy()\n",
    "test_data[\"Rank\"] = test_data.index + 1\n",
    "print(test_data[\"team identifier\"])\n",
    "# Normalize the features in the training data\n",
    "\n",
    "\n",
    "# Aggregate features by team identifier\n",
    "aggregated_data = train_data.groupby(\"team identifier\").mean().reset_index()\n",
    "# Prepare pairwise comparison data for aggregated data\n",
    "all_pairwise_data = []\n",
    "\n",
    "\n",
    "\n",
    "# Creating pairwise ranking data for each season separately.\n",
    "# ...\n",
    "\n",
    "for dataset in preprocessed_datasets[:-1]:  # Exclude the last dataset (test dataset)\n",
    "    # Drop non-numeric columns\n",
    "    numeric_dataset = dataset.select_dtypes(include=['int64', 'float64']).drop(\"team identifier\", axis=1)\n",
    "\n",
    "    for i, j in combinations(numeric_dataset.index, 2):\n",
    "        feature_diff = numeric_dataset.iloc[i] - numeric_dataset.iloc[j]\n",
    "\n",
    "        # If i < j, then label = 1, indicating team i is ranked higher than team j\n",
    "        label = 1 if i < j else 0\n",
    "        all_pairwise_data.append((feature_diff, label))\n",
    "\n",
    "        # Data Augmentation: Add the reverse pair with the opposite label\n",
    "        feature_diff_reverse = numeric_dataset.iloc[j] - numeric_dataset.iloc[i]\n",
    "        label_reverse = 1 - label\n",
    "        all_pairwise_data.append((feature_diff_reverse, label_reverse))\n",
    "\n",
    "# ...\n",
    "\n",
    "\n",
    "pairwise_X = pd.DataFrame([item[0] for item in all_pairwise_data])\n",
    "pairwise_y = [item[1] for item in all_pairwise_data]\n",
    "print(pairwise_y)\n",
    "\n",
    "# Split the data and convert it to DMatrix\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    pairwise_X,\n",
    "    pairwise_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "\n",
    "# Define the XGBoost hypermodel to only build the parameter dictionary\n",
    "class XGBHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        param = {\n",
    "            'max_depth': hp.Int('max_depth', 3, 10, 1),\n",
    "            'eta': hp.Float('eta', 0.01, 0.5, step=0.01),\n",
    "            'subsample': hp.Float('subsample', 0.5, 1),\n",
    "            'colsample_bytree': hp.Float('colsample_bytree', 0.5, 1),\n",
    "            'gamma': hp.Float('gamma', 0, 5),\n",
    "            'min_child_weight': hp.Int('min_child_weight', 1, 10),\n",
    "            'lambda': hp.Float('lambda', 0.01, 1),\n",
    "            'alpha': hp.Float('alpha', 0.01, 1),\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        return param\n",
    "\n",
    "# Define the custom tuner\n",
    "class XGBTuner(RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        params = self.hypermodel.build(hp)\n",
    "\n",
    "        # Train the model and get evaluation results\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(params, dtrain, evals=[(dval, 'eval')],\n",
    "                        early_stopping_rounds=10, verbose_eval=False, evals_result=evals_result)\n",
    "\n",
    "        # Get the last evaluation result\n",
    "        last_eval = evals_result['eval']['logloss'][-1]  # Changed rmse to logloss\n",
    "\n",
    "        # Report the result to the tuner\n",
    "        self.oracle.update_trial(trial.trial_id, {'val_logloss': last_eval})  # Changed val_rmse to val_logloss\n",
    "        self.save_model(trial.trial_id, bst)\n",
    "\n",
    "    def save_model(self, trial_id, model, step=0):\n",
    "        fname = os.path.join(self.get_trial_dir(trial_id), f'model_{step}.xgb')\n",
    "        model.save_model(fname)\n",
    "\n",
    "    def load_model(self, trial_id, step=0):\n",
    "        fname = os.path.join(self.get_trial_dir(trial_id), f'model_{step}.xgb')\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(fname)\n",
    "        return model\n",
    "\n",
    "# Use the custom tuner to search for the best hyperparameters\n",
    "tuner = XGBTuner(\n",
    "    XGBHyperModel(),\n",
    "    objective=Objective('val_logloss', direction='min'),\n",
    "    max_trials=50,\n",
    "    directory='xgb_tuner',\n",
    "    project_name='xgb_tuning'\n",
    ")\n",
    "\n",
    "tuner.search()\n",
    "# Display the best hyperparameters\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best max_depth:\", best_hp.get('max_depth'))\n",
    "print(\"Best eta:\", best_hp.get('eta'))\n",
    "print(\"Best subsample:\", best_hp.get('subsample'))\n",
    "print(\"Best colsample_bytree:\", best_hp.get('colsample_bytree'))\n",
    "print(\"Best gamma:\", best_hp.get('gamma'))\n",
    "print(\"Best min_child_weight:\", best_hp.get('min_child_weight'))\n",
    "print(\"Best lambda (L2 regularization):\", best_hp.get('lambda'))\n",
    "print(\"Best alpha (L1 regularization):\", best_hp.get('alpha'))\n",
    "best_params = {\n",
    "    'max_depth': best_hp.get('max_depth'),\n",
    "    'eta': best_hp.get('eta'),\n",
    "    'subsample': best_hp.get('subsample'),\n",
    "    'colsample_bytree': best_hp.get('colsample_bytree'),\n",
    "    'gamma': best_hp.get('gamma'),\n",
    "    'min_child_weight': best_hp.get('min_child_weight'),\n",
    "    'lambda': best_hp.get('lambda'),\n",
    "    'alpha': best_hp.get('alpha'),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "num_round = 60\n",
    "bst_aggregated = xgb.train(best_params, dtrain, num_round, evals=[(dval, 'eval')], early_stopping_rounds=10)\n",
    "\n",
    "bst_aggregated.save_model('XGBoosttrainingSetFive.xgb')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T09:25:13.727143200Z"
    }
   },
   "id": "d986a31089c94f48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-22T09:25:13.729150500Z"
    }
   },
   "id": "cb65f38af2df9849"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T09:25:13.732496100Z",
     "start_time": "2023-09-22T09:25:13.730140700Z"
    }
   },
   "id": "4c3c4fd5c709ed35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
