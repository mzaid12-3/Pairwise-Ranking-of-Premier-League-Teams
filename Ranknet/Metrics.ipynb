{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-07T18:41:30.567053400Z",
     "start_time": "2023-11-07T18:41:30.530298800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 14, 11, 30, 27, 5, 29, 25, 1, 15, 16, 22, 12, 8, 6, 7, 19, 2, 23, 9]\n",
      "[11, 8, 3, 14, 30, 22, 7, 5, 15, 27, 19, 12, 6, 2, 1, 16, 23, 29, 25, 9]\n",
      "[3, 14, 11, 30, 27, 5, 29, 25, 1, 15, 16, 22, 12, 8, 6, 7, 19, 2, 23, 9]\n",
      "Accuracy: 5.00%\n",
      "NDCG@5: 0.56\n",
      "Mean Average Precision (up to rank 5): 0.44\n",
      "NDCG@10: 0.56\n",
      "Mean Average Precision (up to rank 10): 0.53\n",
      "NDCG@15: 0.65\n",
      "Mean Average Precision (up to rank 15): 0.59\n",
      "NDCG@20: 0.65\n",
      "Mean Average Precision (up to rank 20): 0.68\n"
     ]
    }
   ],
   "source": [
    "#2018/19\n",
    "from math import log2\n",
    "with open('predicted_rankingsTrainingSetOne.txt', 'r') as f:\n",
    "    predicted_ranks_array = [int(line.strip()) for line in f.readlines()]\n",
    "print(predicted_ranks_array)\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "7,\n",
    "5,\n",
    "15,\n",
    "27,\n",
    "19,\n",
    "12,\n",
    "6,\n",
    "2,\n",
    "1,\n",
    "16,\n",
    "23,\n",
    "29,\n",
    "25,\n",
    "9\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "predicted_rankings = predicted_ranks_array\n",
    "print(predicted_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    print(relevance)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    " \n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 11, 14, 5, 27, 30, 15, 22, 28, 6, 1, 19, 35, 2, 12, 16, 8, 7, 10, 23]\n",
      "[8, 11, 22, 3, 15, 14, 7, 30, 10, 1, 16, 5, 6, 12, 23, 27, 35, 2, 19, 28]\n",
      "Accuracy: 5.00%\n",
      "NDCG@20: 0.52\n",
      "NDCG@5: 0.30\n",
      "Mean Average Precision (up to rank 5): 0.35\n",
      "NDCG@10: 0.36\n",
      "Mean Average Precision (up to rank 10): 0.48\n",
      "NDCG@15: 0.36\n",
      "Mean Average Precision (up to rank 15): 0.54\n",
      "NDCG@20: 0.52\n",
      "Mean Average Precision (up to rank 20): 0.63\n"
     ]
    }
   ],
   "source": [
    "#2019/20\n",
    "with open('predicted_rankingsTrainingSetTwo.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "\n",
    "print(predicted_rankings)\n",
    "actual_rankings = [8,\n",
    "11,\n",
    "22,\n",
    "3,\n",
    "15,\n",
    "14,\n",
    "7,\n",
    "30,\n",
    "10,\n",
    "1,\n",
    "16,\n",
    "5,\n",
    "6,\n",
    "12,\n",
    "23,\n",
    "27,\n",
    "35,\n",
    "2,\n",
    "19,\n",
    "28\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k = 20\n",
    "\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:24:49.972708900Z",
     "start_time": "2023-10-13T20:24:49.909262100Z"
    }
   },
   "id": "ad2b498a97476f17"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 3, 21, 11, 27, 30, 25, 5, 15, 22, 6, 1, 16, 35, 8, 12, 18, 7, 23, 10]\n",
      "[11, 22, 8, 3, 15, 27, 14, 30, 18, 5, 35, 6, 7, 12, 16, 23, 1, 25, 21, 10]\n",
      "Accuracy: 5.00%\n",
      "NDCG@20: 0.50\n",
      "NDCG@5: 0.35\n",
      "Mean Average Precision (up to rank 5): 0.18\n",
      "NDCG@10: 0.46\n",
      "Mean Average Precision (up to rank 10): 0.41\n",
      "NDCG@15: 0.50\n",
      "Mean Average Precision (up to rank 15): 0.52\n",
      "NDCG@20: 0.50\n",
      "Mean Average Precision (up to rank 20): 0.62\n"
     ]
    }
   ],
   "source": [
    "#2020/21\n",
    "with open('predicted_rankingsTrainingSetThree.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "\n",
    "print(predicted_rankings)\n",
    "actual_rankings = [11,\n",
    "22,\n",
    "8,\n",
    "3,\n",
    "15,\n",
    "27,\n",
    "14,\n",
    "30,\n",
    "18,\n",
    "5,\n",
    "35,\n",
    "6,\n",
    "7,\n",
    "12,\n",
    "16,\n",
    "23,\n",
    "1,\n",
    "25,\n",
    "21,\n",
    "10\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k = 20\n",
    "\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:25:40.485987200Z",
     "start_time": "2023-10-13T20:25:40.419024700Z"
    }
   },
   "id": "a9399dbe236332a0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 22, 11, 27, 12, 35, 15, 16, 30, 19, 8, 6, 1, 14, 28, 23, 7, 33, 18]\n",
      "[11, 8, 3, 14, 30, 22, 27, 15, 23, 7, 6, 12, 33, 35, 16, 5, 18, 1, 19, 28]\n",
      "Accuracy: 5.00%\n",
      "NDCG@20: 0.58\n",
      "NDCG@5: 0.46\n",
      "Mean Average Precision (up to rank 5): 0.25\n",
      "NDCG@10: 0.47\n",
      "Mean Average Precision (up to rank 10): 0.41\n",
      "NDCG@15: 0.58\n",
      "Mean Average Precision (up to rank 15): 0.50\n",
      "NDCG@20: 0.58\n",
      "Mean Average Precision (up to rank 20): 0.60\n"
     ]
    }
   ],
   "source": [
    "#2021/22\n",
    "\n",
    "with open('predicted_rankingsTrainingSetFour.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "\n",
    "print(predicted_rankings)\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "27,\n",
    "15,\n",
    "23,\n",
    "7,\n",
    "6,\n",
    "12,\n",
    "33,\n",
    "35,\n",
    "16,\n",
    "5,\n",
    "18,\n",
    "1,\n",
    "19,\n",
    "28\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k = 20\n",
    "\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:26:31.939209100Z",
     "start_time": "2023-10-13T20:26:31.902448800Z"
    }
   },
   "id": "45e0d50564cfcb25"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 11, 22, 27, 12, 30, 16, 8, 15, 35, 6, 14, 2, 25, 23, 33, 7, 32, 18]\n",
      "[11, 30, 22, 6, 8, 23, 35, 14, 33, 25, 12, 3, 7, 27, 2, 32, 5, 15, 18, 16]\n",
      "Accuracy: 0.00%\n",
      "NDCG@20: 0.55\n",
      "NDCG@5: 0.40\n",
      "Mean Average Precision (up to rank 5): 0.25\n",
      "NDCG@10: 0.52\n",
      "Mean Average Precision (up to rank 10): 0.32\n",
      "NDCG@15: 0.54\n",
      "Mean Average Precision (up to rank 15): 0.44\n",
      "NDCG@20: 0.55\n",
      "Mean Average Precision (up to rank 20): 0.56\n"
     ]
    }
   ],
   "source": [
    "#2022/23\n",
    "\n",
    "with open('predicted_rankingsTrainingSetFive.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "\n",
    "print(predicted_rankings)\n",
    "actual_rankings = [11,\n",
    "30,\n",
    "22,\n",
    "6,\n",
    "8,\n",
    "23,\n",
    "35,\n",
    "14,\n",
    "33,\n",
    "25,\n",
    "12,\n",
    "3,\n",
    "7,\n",
    "27,\n",
    "2,\n",
    "32,\n",
    "5,\n",
    "15,\n",
    "18,\n",
    "16\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k = 20\n",
    "\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:27:22.455463400Z",
     "start_time": "2023-10-13T20:27:22.411595100Z"
    }
   },
   "id": "9f493a983c7f4eec"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T09:29:01.193507900Z",
     "start_time": "2023-09-22T09:29:01.177512900Z"
    }
   },
   "id": "c3bad71cac7e8932"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
