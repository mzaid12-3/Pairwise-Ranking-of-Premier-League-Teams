{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1a285ef079ef40ae"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# from data_preprocessing import preprocess_data\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "# import tensorflow as tf\n",
    "# \n",
    "# # Load the data\n",
    "# files = [\n",
    "#         \"../My_datasets/2012_13.xlsx\",\n",
    "#         \"../My_datasets/2013_14.xlsx\",\n",
    "#         \"../My_datasets/2014_15.xlsx\",\n",
    "#         \"../My_datasets/2015_16.xlsx\",\n",
    "#         \"../My_datasets/2016_17.xlsx\",\n",
    "#         \"../My_datasets/2017_18.xlsx\",\n",
    "#         \"../My_datasets/2018_19.xlsx\"\n",
    "# \n",
    "# \n",
    "#     ]\n",
    "# datasets = [pd.read_excel(file) for file in files]\n",
    "# \n",
    "#     # Preprocess datasets\n",
    "# numerical_cols_data = datasets[0].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# numerical_cols_data = [col for col in numerical_cols_data if col != \"team identifier\"]\n",
    "# categorical_cols_data = datasets[0].select_dtypes(include=['object']).columns.tolist()\n",
    "# categorical_cols_data = [col for col in categorical_cols_data if col != \"Team\"]\n",
    "# preprocessed_datasets = [preprocess_data(df, numerical_cols_data, categorical_cols_data) for df in datasets]\n",
    "# \n",
    "#     # Prepare test data\n",
    "# test_data = preprocessed_datasets[-1][[\"Team\", \"team identifier\"]].copy()\n",
    "# test_data[\"Rank\"] = test_data.index + 1\n",
    "# \n",
    "#     # Aggregate features by team identifier\n",
    "# train_data = pd.concat(preprocessed_datasets[:-1], ignore_index=True)\n",
    "# aggregated_data = train_data.groupby(\"team identifier\").mean().reset_index()\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# # Create position embedding for the teams\n",
    "# team_to_id = {team: idx for idx, team in enumerate(aggregated_data['team identifier'].unique(), start=1)}\n",
    "# n_teams = len(team_to_id)\n",
    "# position_embedding_dim = X.shape[1]  # Using the feature dimension as embedding dimension\n",
    "# \n",
    "# # Create an input for raw features\n",
    "# input_features = Input(shape=(X.shape[1],), name=\"input_features\")\n",
    "# \n",
    "# # Create an input for position\n",
    "# input_position = Input(shape=(2,), name=\"input_position\")\n",
    "# \n",
    "# # Use an embedding layer for position embedding\n",
    "# # Adjusting the input dimension for the embedding layer based on the maximum value in team_pairs\n",
    "# max_team_id = np.max(team_pairs)\n",
    "# position_embedding = Embedding(input_dim=max_team_id + 1, output_dim=position_embedding_dim, name=\"position_embedding\")(input_position)\n",
    "# \n",
    "# \n",
    "# # Take mean along the teams dimension\n",
    "# position_embedding_mean = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(position_embedding)\n",
    "# \n",
    "# # Concatenate raw features and position embedding\n",
    "# combined_input = Concatenate(axis=-1)([input_features, position_embedding_mean])\n",
    "# \n",
    "# # Use a dense layer to combine them\n",
    "# dense_combined = Dense(units=X.shape[1], activation='relu')(combined_input)\n",
    "# \n",
    "# \n",
    "# # Model just for the input layer\n",
    "# input_model = Model(inputs=[input_features, input_position], outputs=dense_combined)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.607194900Z",
     "start_time": "2023-09-21T08:35:10.321034300Z"
    }
   },
   "id": "50f474f019eb9cb4"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# from keras.src.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Dropout\n",
    "# from kerastuner import HyperModel, RandomSearch\n",
    "# from creatingPairsForTrainingSetOne import trainingSetOne\n",
    "# from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate\n",
    "# import numpy as np\n",
    "# \n",
    "# combined_pairs_trainingSetOne, combined_labels_trainingSetOne, _, team_pairs = trainingSetOne()\n",
    "# X= combined_pairs_trainingSetOne\n",
    "# X = X.reshape(1140, -1)\n",
    "# # Parameters for the transformer block\n",
    "# d_model = X.shape[1]  # Dimension of the input vectors\n",
    "# # num_heads = 4  # Number of attention heads\n",
    "# # ff_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "# # dropout_rate = 0.1  # Dropout rate\n",
    "# # num_transformer_blocks = 2  # Number of transformer blocks\n",
    "# \n",
    "# # Define a Transformer block\n",
    "# def transformer_block(inputs, num_heads, ff_dim, dropout_rate): \n",
    "#     # Add a sequence dimension\n",
    "#     inputs = tf.expand_dims(inputs, 1)\n",
    "# \n",
    "#     # Multi-head Self Attention\n",
    "#     attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)\n",
    "#     attention_output = Dropout(dropout_rate)(attention_output)\n",
    "#     out1 = Add()([inputs, attention_output])\n",
    "#     out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "# \n",
    "#     # Feed-forward network\n",
    "#     ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "#     ffn_output = Dense(inputs.shape[-1])(ffn_output)  # Set the output dimension to match the input dimension\n",
    "#     ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "#     out2 = Add()([out1, ffn_output])\n",
    "#     out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "# \n",
    "#     # Remove the sequence dimension\n",
    "#     out2 = tf.squeeze(out2, 1)\n",
    "# \n",
    "#     return out2\n",
    "# \n",
    "# \n",
    "# class TransformerDualInputHyperModel(HyperModel):\n",
    "#     def __init__(self, input_shape1, input_shape2):\n",
    "#         self.input_shape1 = input_shape1\n",
    "#         self.input_shape2 = input_shape2\n",
    "# \n",
    "#     def build(self, hp):\n",
    "#         input1 = tf.keras.Input(shape=self.input_shape1, name=\"input1\")\n",
    "#         input2 = tf.keras.Input(shape=self.input_shape2, name=\"input2\")\n",
    "#         \n",
    "#         flattened_input1 = tf.keras.layers.Flatten()(input1)  # Flattening the player pairs\n",
    "#         \n",
    "#         merged_input = tf.keras.layers.Concatenate(axis=-1)([flattened_input1, input2])\n",
    "#         \n",
    "#         # Hyperparameters\n",
    "#         # num_heads = 4  # Number of attention heads\n",
    "#         # ff_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "#         num_heads = hp.Int('num_heads', 2, 4, step=2)\n",
    "#         ff_dim = hp.Int('ff_dim', 128, 256, step=64)\n",
    "#         dropout_rate = hp.Float('dropout_rate', 0.0, 0.2, step=0.1)\n",
    "#         num_transformer_blocks = hp.Int('num_transformer_blocks', 1, 2)\n",
    "# \n",
    "#         x = merged_input\n",
    "#         for _ in range(num_transformer_blocks):\n",
    "#             x = transformer_block(x, num_heads, ff_dim, dropout_rate)\n",
    "# \n",
    "#         # Here, you can add your output layers based on the task, for example:\n",
    "#         outputs = tf.keras.layers.Dense(30, activation='softmax')(x)  # Since there are 30 classes\n",
    "# \n",
    "#         model = tf.keras.Model(inputs=[input1, input2], outputs=outputs)\n",
    "#         model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# \n",
    "#         return model\n",
    "# \n",
    "# # Assume X_train is your input data\n",
    "# encoded_labels = to_categorical(combined_labels_trainingSetOne, num_classes=30)\n",
    "# team_pairs = np.array(team_pairs)\n",
    "# \n",
    "# (train_pairs, val_pairs, train_teams, val_teams, train_labels, val_labels) = train_test_split(\n",
    "#     combined_pairs_trainingSetOne,\n",
    "#     team_pairs,  # Include team_pairs in the split\n",
    "#     encoded_labels,\n",
    "#     test_size=0.1,\n",
    "#     random_state=42\n",
    "# )\n",
    "# \n",
    "# input_shape1 = (2, 51)  \n",
    "# input_shape2 = (2,)  \n",
    "# dual_hypermodel = TransformerDualInputHyperModel(input_shape1, input_shape2)\n",
    "# \n",
    "# tuner = RandomSearch(\n",
    "#     dual_hypermodel,\n",
    "#     objective='val_accuracy',\n",
    "#     max_trials=5,\n",
    "#     executions_per_trial=1\n",
    "# )\n",
    "# \n",
    "# tuner.search(\n",
    "#     [train_pairs, train_teams], train_labels,\n",
    "#     validation_data=([val_pairs, val_teams], val_labels),\n",
    "#     batch_size=30, epochs=20\n",
    "# )\n",
    "# \n",
    "# # 1. Get the best hyperparameters\n",
    "# best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# \n",
    "# # 2. Print the best hyperparameters\n",
    "# print(\"Best hyperparameters found:\")\n",
    "# for param, value in best_hp.values.items():\n",
    "#     print(f\"{param}: {value}\")\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.607194900Z",
     "start_time": "2023-09-21T08:35:10.330577100Z"
    }
   },
   "id": "f5bb9968d37de58d"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# from creatingPairsForTrainingSetOne import trainingSetOne\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# \n",
    "# \n",
    "# best_hp = tuner.get_best_hyperparameters()[0]\n",
    "# \n",
    "# \n",
    "# best_model = dual_hypermodel.build(best_hp)\n",
    "# \n",
    "# \n",
    "# combined_pairs_trainingSetOne, combined_labels_trainingSetOne, _, team_pairs = trainingSetOne()\n",
    "# encoded_labels = to_categorical(combined_labels_trainingSetOne, num_classes=30)\n",
    "# print(combined_labels_trainingSetOne.shape)\n",
    "# team_pairs = np.array(team_pairs)\n",
    "# best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# best_model.fit([combined_pairs_trainingSetOne, team_pairs], encoded_labels, epochs=20, batch_size=30)\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.610704100Z",
     "start_time": "2023-09-21T08:35:10.349593100Z"
    }
   },
   "id": "5efa753af926f3a8"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# # Assume test_pairs are your input pairs for which you want predictions\n",
    "# predictions = best_model.predict([combined_pairs_trainingSetOne, team_pairs])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.611715300Z",
     "start_time": "2023-09-21T08:35:10.363306300Z"
    }
   },
   "id": "77d0af81da0b7910"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# # Given the predictions\n",
    "# # Calculate the average probability for each team\n",
    "# average_probabilities = predictions.mean(axis=0)\n",
    "# \n",
    "# # Team identifiers\n",
    "# team_ids = [11, 8, 3, 14, 30, 22, 7, 5, 15, 27, 19, 12, 6, 2, 1, 16, 23, 29, 25, 9]\n",
    "# \n",
    "# # Sort teams based on average probabilities\n",
    "# sorted_teams = [team for _, team in sorted(zip(average_probabilities, team_ids), reverse=True)]\n",
    "# sorted_teams_probs = sorted(zip(team_ids, average_probabilities), key=lambda x: x[1], reverse=True)\n",
    "# \n",
    "# print(sorted_teams_probs)\n",
    "# sorted_teams"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.611715300Z",
     "start_time": "2023-09-21T08:35:10.381464600Z"
    }
   },
   "id": "d9af00a804114ffd"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# \n",
    "# files = [\n",
    "#     \"../My_datasets/2012_13.xlsx\",\n",
    "#     \"../My_datasets/2013_14.xlsx\",\n",
    "#     \"../My_datasets/2014_15.xlsx\",\n",
    "#     \"../My_datasets/2015_16.xlsx\",\n",
    "#     \"../My_datasets/2016_17.xlsx\",\n",
    "#     \"../My_datasets/2017_18.xlsx\",\n",
    "#     \n",
    "# ]\n",
    "# \n",
    "# season_standings = []\n",
    "# \n",
    "# # Reading and processing each Excel file\n",
    "# for file in files:\n",
    "#     df = pd.read_excel(file)\n",
    "#     # Extract the team identifier and its position based on index\n",
    "#     standings_dict = {row['team identifier']: idx+1 for idx, row in df.iterrows()}\n",
    "#     season_standings.append(standings_dict)\n",
    "# \n",
    "# print(season_standings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.611715300Z",
     "start_time": "2023-09-21T08:35:10.422218300Z"
    }
   },
   "id": "b8cc3b91cefc10f4"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Your provided data and model predictions\n",
    "# predictions = best_model.predict([combined_pairs_trainingSetOne, team_pairs])\n",
    "#  \n",
    "# average_probabilities = predictions.mean(axis=0)\n",
    "# print(average_probabilities)\n",
    "# team_ids = [11, 8, 3, 14, 30, 22, 7, 5, 15, 27, 19, 12, 6, 2, 1, 16, 23, 29, 25, 9]\n",
    "# \n",
    "# \n",
    "# season_weights = [1, 2, 3, 4, 5, 6]\n",
    "# weighted_standings = []\n",
    "# \n",
    "# for team_id in team_ids:\n",
    "#     team_weighted_standing = sum(season_standings[season].get(team_id, len(team_ids)+1) * season_weights[season] for season in range(6))\n",
    "#     weighted_standings.append(team_weighted_standing)\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Normalize the weighted standings\n",
    "# max_weighted_value = len(team_ids) * sum(season_weights)\n",
    "# normalized_standings = [(max_weighted_value - standing) / max_weighted_value for standing in weighted_standings]\n",
    "# \n",
    "# # Weights for combining average probabilities and standings\n",
    "# w1 = 0.7\n",
    "# w2 = 0.3\n",
    "# \n",
    "# # Adjust the average probabilities\n",
    "# # Adjust only the probabilities corresponding to the 20 teams\n",
    "# adjusted_probabilities = np.array(average_probabilities)\n",
    "# adjusted_probabilities[:20] = w1 * average_probabilities[:20] + w2 * np.array(normalized_standings)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# # Sort teams based on adjusted probabilities\n",
    "# sorted_teams = [team for _, team in sorted(zip(adjusted_probabilities, team_ids), reverse=True)]\n",
    "# sorted_teams_probs = sorted(zip(team_ids, adjusted_probabilities), key=lambda x: x[1], reverse=True)\n",
    "# \n",
    "# print(sorted_teams_probs)\n",
    "# print(sorted_teams)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.611715300Z",
     "start_time": "2023-09-21T08:35:10.428235300Z"
    }
   },
   "id": "4e10349d17ca9038"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Predictions from the model for pairs of teams\n",
    "# predictions = best_model.predict([combined_pairs_trainingSetOne, team_pairs])\n",
    "# \n",
    "# # Team identifiers\n",
    "# team_ids = [11, 8, 3, 14, 30, 22, 7, 5, 15, 27, 19, 12, 6, 2, 1, 16, 23, 29, 25, 9]\n",
    "# # print(team_pairs[:5])\n",
    "# # Extract and aggregate scores for individual teams from pair predictions\n",
    "# team_scores = {team_id: 0 for team_id in team_ids}\n",
    "# for idx, pair in enumerate(team_pairs):\n",
    "#     team1 = pair[0]\n",
    "#     team2 = pair[1]\n",
    "#     \n",
    "#     try:\n",
    "#         team_scores[team1] += predictions[idx]\n",
    "#         print(team_scores[team1],\":\",predictions[idx])\n",
    "#         team_scores[team2] += predictions[idx]\n",
    "#     except KeyError:\n",
    "#         print(f\"Team identifier {team2} not found in team_scores dictionary.\")\n",
    "#         if team2 not in team_ids:\n",
    "#             print(f\"Team identifier {team2} is also not in the original team_ids list.\")\n",
    "# \n",
    "# # Normalize these aggregated scores to get average probabilities\n",
    "# total_matches = len(combined_pairs_trainingSetOne) / 2  # Each team appears in half the pairs\n",
    "# average_probabilities = {team_id: score / total_matches for team_id, score in team_scores.items()}\n",
    "# \n",
    "# # Weights for standings\n",
    "# season_weights = [1, 2, 3, 4, 5, 6]\n",
    "# weighted_standings = []\n",
    "# for team_id in team_ids:\n",
    "#     team_weighted_standing = sum(season_standings[season].get(team_id, len(team_ids)+1) * season_weights[season] for season in range(6))\n",
    "#     weighted_standings.append(team_weighted_standing)\n",
    "# \n",
    "# # Normalize the weighted standings\n",
    "# max_weighted_value = len(team_ids) * sum(season_weights)\n",
    "# normalized_standings = [(max_weighted_value - standing) / max_weighted_value for standing in weighted_standings]\n",
    "# \n",
    "# # Weights for combining average probabilities and standings\n",
    "# w1 = 0.7\n",
    "# w2 = 0.3\n",
    "# \n",
    "# # Adjust the average probabilities\n",
    "# adjusted_scores = {team_id: w1 * average_probabilities[team_id] + w2 * normalized_standings[team_ids.index(team_id)] for team_id in team_ids}\n",
    "# for team_id, score in adjusted_scores.items():\n",
    "#     print(f\"Team ID: {team_id}, Score: {score}\")\n",
    "# \n",
    "# # Sort teams based on adjusted scores\n",
    "# scalar_scores = {team_id: np.sum(score) for team_id, score in adjusted_scores.items()}\n",
    "# \n",
    "# # Sort teams based on these scalar scores\n",
    "# sorted_teams = [team for team, _ in sorted(scalar_scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "# sorted_teams_scores = sorted(scalar_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "# \n",
    "# print(sorted_teams_scores)\n",
    "# print(sorted_teams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.611715300Z",
     "start_time": "2023-09-21T08:35:10.447444Z"
    }
   },
   "id": "cd33f4eb1da21ea6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 3, 14, 30, 22, 7, 5, 15, 27, 19, 12, 6, 2, 1, 16, 23, 29, 25, 9]\n",
      "Accuracy: 35.00%\n",
      "NDCG@5: 0.82\n",
      "Mean Average Precision (up to rank 5): 0.74\n",
      "NDCG@10: 0.93\n",
      "Mean Average Precision (up to rank 10): 0.82\n",
      "NDCG@15: 0.93\n",
      "Mean Average Precision (up to rank 15): 0.84\n",
      "NDCG@20: 0.93\n",
      "Mean Average Precision (up to rank 20): 0.86\n"
     ]
    }
   ],
   "source": [
    "#2018/19\n",
    "from math import log2\n",
    "with open('predicted_rankingsTrainingSetOne.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "7,\n",
    "5,\n",
    "15,\n",
    "27,\n",
    "19,\n",
    "12,\n",
    "6,\n",
    "2,\n",
    "1,\n",
    "16,\n",
    "23,\n",
    "29,\n",
    "25,\n",
    "9\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing mAP\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:57:03.358997700Z",
     "start_time": "2023-10-13T19:57:03.338568100Z"
    }
   },
   "id": "f8329193154f062c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 14, 3, 8, 30, 22, 5, 15, 27, 16, 12, 2, 6, 19, 1, 7, 23, 35, 10, 28]\n",
      "[8, 11, 22, 3, 15, 14, 7, 30, 10, 1, 16, 5, 6, 12, 23, 27, 35, 2, 19, 28]\n",
      "Accuracy: 10.00%\n",
      "NDCG@5: 0.67\n",
      "Mean Average Precision (up to rank 5): 0.44\n",
      "NDCG@10: 0.73\n",
      "Mean Average Precision (up to rank 10): 0.61\n",
      "NDCG@15: 0.73\n",
      "Mean Average Precision (up to rank 15): 0.66\n",
      "NDCG@20: 0.73\n",
      "Mean Average Precision (up to rank 20): 0.73\n"
     ]
    }
   ],
   "source": [
    "#2019/20\n",
    "with open('predicted_rankingsTrainingSetTwo.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "\n",
    "print(predicted_rankings)\n",
    "actual_rankings = [8,\n",
    "11,\n",
    "22,\n",
    "3,\n",
    "15,\n",
    "14,\n",
    "7,\n",
    "30,\n",
    "10,\n",
    "1,\n",
    "16,\n",
    "5,\n",
    "6,\n",
    "12,\n",
    "23,\n",
    "27,\n",
    "35,\n",
    "2,\n",
    "19,\n",
    "28\n",
    "\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing mAP\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:58:34.972008300Z",
     "start_time": "2023-10-13T19:58:34.906588100Z"
    }
   },
   "id": "c98af26d557aca21"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 22, 8, 3, 15, 27, 14, 30, 18, 5, 35, 6, 7, 12, 16, 23, 1, 25, 21, 10]\n",
      "Accuracy: 15.00%\n",
      "NDCG@5: 0.94\n",
      "Mean Average Precision (up to rank 5): 0.74\n",
      "NDCG@10: 0.94\n",
      "Mean Average Precision (up to rank 10): 0.77\n",
      "NDCG@15: 0.94\n",
      "Mean Average Precision (up to rank 15): 0.79\n",
      "NDCG@20: 0.94\n",
      "Mean Average Precision (up to rank 20): 0.82\n"
     ]
    }
   ],
   "source": [
    "#2020/21\n",
    "\n",
    "with open('predicted_rankingsTrainingSetThree.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "actual_rankings =[11,\n",
    "22,\n",
    "8,\n",
    "3,\n",
    "15,\n",
    "27,\n",
    "14,\n",
    "30,\n",
    "18,\n",
    "5,\n",
    "35,\n",
    "6,\n",
    "7,\n",
    "12,\n",
    "16,\n",
    "23,\n",
    "1,\n",
    "25,\n",
    "21,\n",
    "10\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_rankings):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing mAP\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T19:59:54.171000700Z",
     "start_time": "2023-10-13T19:59:54.101233100Z"
    }
   },
   "id": "8dd8eb91ed2047ff"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 22, 3, 14, 30, 15, 5, 27, 16, 12, 6, 1, 7, 19, 35, 23, 18, 33, 28]\n",
      "[11, 8, 3, 14, 30, 22, 27, 15, 23, 7, 6, 12, 33, 35, 16, 5, 18, 1, 19, 28]\n",
      "Accuracy: 15.00%\n",
      "NDCG@5: 0.98\n",
      "Mean Average Precision (up to rank 5): 0.84\n",
      "NDCG@10: 0.99\n",
      "Mean Average Precision (up to rank 10): 0.86\n",
      "NDCG@15: 0.99\n",
      "Mean Average Precision (up to rank 15): 0.84\n",
      "NDCG@20: 0.99\n",
      "Mean Average Precision (up to rank 20): 0.86\n"
     ]
    }
   ],
   "source": [
    "#2021/22\n",
    "# Load the predicted rankings\n",
    "with open('predicted_rankingsTrainingSetFour.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Print the loaded predicted rankings\n",
    "print(predicted_rankings)\n",
    "\n",
    "# Define the actual rankings\n",
    "actual_rankings = [11,\n",
    "8,\n",
    "3,\n",
    "14,\n",
    "30,\n",
    "22,\n",
    "27,\n",
    "15,\n",
    "23,\n",
    "7,\n",
    "6,\n",
    "12,\n",
    "33,\n",
    "35,\n",
    "16,\n",
    "5,\n",
    "18,\n",
    "1,\n",
    "19,\n",
    "28\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "# Compute and print the accuracy\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Define relevance scores for DCG calculation\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing mAP\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:06:23.375997800Z",
     "start_time": "2023-10-13T20:06:23.339566400Z"
    }
   },
   "id": "e4d4518161ea5a04"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 14, 3, 8, 30, 22, 5, 15, 27, 16, 12, 6, 2, 7, 23, 35, 25, 18, 33, 32]\n",
      "[11, 30, 22, 6, 8, 23, 35, 14, 33, 25, 12, 3, 7, 27, 2, 32, 5, 15, 18, 16]\n",
      "Accuracy: 15.00%\n",
      "NDCG@5: 0.81\n",
      "Mean Average Precision (up to rank 5): 0.54\n",
      "NDCG@10: 0.85\n",
      "Mean Average Precision (up to rank 10): 0.56\n",
      "NDCG@15: 0.88\n",
      "Mean Average Precision (up to rank 15): 0.60\n",
      "NDCG@20: 0.88\n",
      "Mean Average Precision (up to rank 20): 0.67\n"
     ]
    }
   ],
   "source": [
    "#2022/23\n",
    "\n",
    "# Load the predicted rankings\n",
    "with open('predicted_rankingsTrainingSetFive.txt', 'r') as f:\n",
    "    predicted_rankings = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# Print the loaded predicted rankings\n",
    "print(predicted_rankings)\n",
    "\n",
    "# Define the actual rankings\n",
    "actual_rankings =  [11,\n",
    "30,\n",
    "22,\n",
    "6,\n",
    "8,\n",
    "23,\n",
    "35,\n",
    "14,\n",
    "33,\n",
    "25,\n",
    "12,\n",
    "3,\n",
    "7,\n",
    "27,\n",
    "2,\n",
    "32,\n",
    "5,\n",
    "15,\n",
    "18,\n",
    "16\n",
    "\n",
    "\n",
    "]\n",
    "print(actual_rankings)\n",
    "\n",
    "# Compute and print the accuracy\n",
    "count_matches = 0\n",
    "for a, p in zip(actual_rankings, predicted_ranks_array):\n",
    "    if a == p:\n",
    "        count_matches += 1\n",
    "accuracy = count_matches / len(actual_rankings)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Define relevance scores for DCG calculation\n",
    "relevance_scores = {}\n",
    "for rank, team in enumerate(actual_rankings):\n",
    "    relevance_scores[team] = 19 - rank\n",
    "\n",
    "def dcg_at_k(predicted_ranks, k):\n",
    "    dcg = 0\n",
    "    for idx, team in enumerate(predicted_ranks[:k]):\n",
    "        dcg += (2**relevance_scores[team] - 1) / log2(idx + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(predicted_ranks, k):\n",
    "    best_dcg = dcg_at_k(sorted(actual_rankings, key=lambda x: relevance_scores[x], reverse=True), k)\n",
    "    actual_dcg = dcg_at_k(predicted_ranks, k)\n",
    "    return actual_dcg / best_dcg\n",
    "\n",
    "def compute_relevance(predicted_rankings, actual_rankings):\n",
    "    \"\"\"Compute continuous relevance values based on rank difference.\"\"\"\n",
    "    relevance = []\n",
    "    for predicted_team in predicted_rankings:\n",
    "        predicted_rank = predicted_rankings.index(predicted_team)\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        # The relevance is inversely proportional to rank difference\n",
    "        rel = 1 / (abs(predicted_rank - actual_rank) + 1)\n",
    "        relevance.append(rel)\n",
    "    return relevance\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "# Sample rankings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_true_and_false_positives(predicted_rankings, actual_rankings, k):\n",
    "    \"\"\"Compute True Positives and False Positives up to rank k.\"\"\"\n",
    "    true_positives = 0\n",
    "    for i in range(k):\n",
    "        predicted_team = predicted_rankings[i]\n",
    "        actual_rank = actual_rankings.index(predicted_team)\n",
    "        if actual_rank < k:\n",
    "            true_positives += 1\n",
    "    false_positives = k - true_positives\n",
    "    return true_positives, false_positives\n",
    "\n",
    "def average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Average Precision up to rank k.\"\"\"\n",
    "    true_positives, false_positives = compute_true_and_false_positives(predicted_rankings, actual_rankings, k)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def mean_average_precision(predicted_rankings, actual_rankings, k=20):\n",
    "    \"\"\"Compute Mean Average Precision up to rank k.\"\"\"\n",
    "    return sum(average_precision(predicted_rankings, actual_rankings, k=i) for i in range(1, k+1)) / k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing mAP\n",
    "k = 5\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "\n",
    "k = 10\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "k = 15\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    "\n",
    "# Computing mAP\n",
    "k = 20\n",
    "map_value = mean_average_precision(predicted_rankings, actual_rankings, k)\n",
    "# print(f'DCG@{k}: {dcg_at_k(predicted_ranks_array, k):.2f}')\n",
    "print(f'NDCG@{k}: {ndcg_at_k(predicted_rankings, k):.2f}')\n",
    "print(f\"Mean Average Precision (up to rank {k}): {map_value:.2f}\")\n",
    " \n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T20:08:37.865442500Z",
     "start_time": "2023-10-13T20:08:37.822827900Z"
    }
   },
   "id": "f18ad9fb5513d764"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T08:35:10.846182400Z",
     "start_time": "2023-09-21T08:35:10.644382200Z"
    }
   },
   "id": "249294d6758ad70c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
